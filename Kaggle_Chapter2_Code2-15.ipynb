{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/15/606f81a2b8a8e82eaa10683cb3f3074905ec65d3bcef949e3f0909f165a5/xgboost-0.80-py2.py3-none-manylinux1_x86_64.whl (15.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 15.8MB 3.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from xgboost) (1.1.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from xgboost) (1.15.1)\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-0.80\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 17s, sys: 11.3 s, total: 1min 29s\n",
      "Wall time: 2min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(2018)\n",
    "\n",
    "trn = pd.read_csv('train_ver2.csv', low_memory=False)\n",
    "tst = pd.read_csv('test_ver2.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.5 s, sys: 23 s, total: 49.5 s\n",
      "Wall time: 49.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prods = trn.columns[24:].tolist()\n",
    "\n",
    "trn[prods] = trn[prods].fillna(0.0).astype(np.int8)\n",
    "\n",
    "no_product = trn[prods].sum(axis=1) == 0\n",
    "trn = trn[~no_product]\n",
    "\n",
    "for col in trn.columns[24:]:\n",
    "    tst[col] = 0\n",
    "    \n",
    "df = pd.concat([trn, tst], axis=0)\n",
    "\n",
    "features = []\n",
    "\n",
    "categorical_cols = ['ind_empleado', 'pais_residencia', 'sexo', 'tiprel_1mes', 'indresi', 'indext',\n",
    "                   'conyuemp', 'canal_entrada', 'indfall', 'tipodom', 'nomprov', 'segmento']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    df[col], _ = df[col].factorize(na_sentinel=-99)\n",
    "features += categorical_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.35 s, sys: 16 ms, total: 3.37 s\n",
      "Wall time: 3.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df['age'].replace(' NA', -99, inplace=True)\n",
    "df['age'] = df['age'].astype(np.int8)\n",
    "\n",
    "df['antiguedad'].replace('     NA', -99, inplace=True)\n",
    "df['antiguedad'] = df['antiguedad'].astype(np.int8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.04 s, sys: 532 ms, total: 4.58 s\n",
      "Wall time: 4.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['renta'].replace('         NA', -99, inplace=True)\n",
    "df['renta'].fillna(-99, inplace=True)\n",
    "df['renta'] = df['renta'].astype(float).astype(np.int8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.57 s, sys: 48 ms, total: 1.62 s\n",
      "Wall time: 1.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['indrel_1mes'].replace('P', 5, inplace=True)\n",
    "df['indrel_1mes'].fillna(-99, inplace=True)\n",
    "df['indrel_1mes'] = df['indrel_1mes'].astype(float).astype(np.int8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 6.44 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "features += ['age', 'antiguedad', 'renta', 'ind_nuevo', 'indrel', 'indrel_1mes', 'ind_actividad_cliente']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.3 s, sys: 696 ms, total: 18 s\n",
      "Wall time: 18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['fecha_alta_month'] = df['fecha_alta'].map(lambda x:0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\n",
    "df['fecha_alta_year'] = df['fecha_alta'].map(lambda x:0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n",
    "features += ['fecha_alta_month', 'fecha_alta_year']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.02 s, sys: 492 ms, total: 6.51 s\n",
      "Wall time: 6.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['ult_fec_cli_1t_month'] = df['ult_fec_cli_1t'].map(lambda x:0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\n",
    "df['ult_fec_cli_1t_year'] = df['ult_fec_cli_1t'].map(lambda x:0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n",
    "features += ['ult_fec_cli_1t_month', 'ult_fec_cli_1t_year']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 37.7 s, total: 1min 38s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.fillna(-99, inplace=True)\n",
    "\n",
    "def date_to_int(str_date):\n",
    "    Y, M, D = [int(a) for a in str_date.strip().split(\"-\")]\n",
    "    int_date = (int(Y) - 2015) * 12 + int(M)\n",
    "    return int_date\n",
    "\n",
    "df['int_date'] = df['fecha_dato'].map(date_to_int).astype(np.int8)\n",
    "\n",
    "df_lag = df.copy()\n",
    "df_lag.columns = [col + '_prev' if col not in ['ncodpers','int_date'] else col for col in df.columns ]\n",
    "df_lag['int_date'] += 1\n",
    "\n",
    "df_trn = df.merge(df_lag, on=['ncodpers', 'int_date'], how='left')\n",
    "\n",
    "del df, df_lag\n",
    "for prod in prods:\n",
    "    prev = prod + '_prev'\n",
    "    df_trn[prev].fillna(0, inplace=True)\n",
    "    \n",
    "df_trn.fillna(-99, inplace=True)\n",
    "\n",
    "features += [feature + '_prev' for feature in features]\n",
    "features += [prod + '_prev' for prod in prods]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.85 s, sys: 2 s, total: 5.84 s\n",
      "Wall time: 5.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "use_dates = ['2016-01-28', '2016-02-28', '2016-03-28', '2016-04-28', '2016-05-28']\n",
    "trn = df_trn[df_trn['fecha_dato'].isin(use_dates)]\n",
    "tst = df_trn[df_trn['fecha_dato'] == '2016-06-28']\n",
    "\n",
    "del df_trn\n",
    "\n",
    "X=[]\n",
    "Y=[]\n",
    "\n",
    "for i, prod in enumerate(prods):\n",
    "    prev = prod + '_prev'\n",
    "    prX = trn[(trn[prod] == 1) & (trn[prev] == 0)]\n",
    "    prY = np.zeros(prX.shape[0], dtype=np.int8) + i\n",
    "    X.append(prX)\n",
    "    Y.append(prY)\n",
    "    \n",
    "XY = pd.concat(X)\n",
    "Y = np.hstack(Y)\n",
    "XY['y'] = Y\n",
    "\n",
    "\n",
    "vld_date = '2016-05-28'\n",
    "XY_trn = XY[XY['fecha_dato'] != vld_date]\n",
    "XY_vld = XY[XY['fecha_dato'] == vld_date]\n",
    "\n",
    "## - TESTING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 124 ms, sys: 144 ms, total: 268 ms\n",
      "Wall time: 266 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## XXGBoost model training\n",
    "\n",
    "\n",
    "param = {\n",
    "    'booster': 'gbtree',\n",
    "    'max_depth': 8,\n",
    "    'nthread': 4,\n",
    "    'num_class': len(prods),\n",
    "    'objective': 'multi:softprob',\n",
    "    'silent': 1,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    \n",
    "    'eta': 0.1,\n",
    "    'min_child_weight': 10,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'colsample_bylevel': 0.9,\n",
    "    'seed': 2018,\n",
    "}\n",
    "\n",
    "X_trn = XY_trn.as_matrix(columns=features)\n",
    "Y_trn = XY_trn.as_matrix(columns=['y'])\n",
    "\n",
    "dtrn = xgb.DMatrix(X_trn, label=Y_trn, feature_names=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vld = XY_vld.as_matrix(columns=features)\n",
    "Y_vld = XY_vld.as_matrix(columns=['y'])\n",
    "\n",
    "dvld = xgb.DMatrix(X_vld, label=Y_vld, feature_names=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.73433\teval-mlogloss:2.74233\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 20 rounds.\n",
      "[1]\ttrain-mlogloss:2.48344\teval-mlogloss:2.49555\n",
      "[2]\ttrain-mlogloss:2.30469\teval-mlogloss:2.31939\n",
      "[3]\ttrain-mlogloss:2.15884\teval-mlogloss:2.17532\n",
      "[4]\ttrain-mlogloss:2.03811\teval-mlogloss:2.05536\n",
      "[5]\ttrain-mlogloss:1.9436\teval-mlogloss:1.96203\n",
      "[6]\ttrain-mlogloss:1.86333\teval-mlogloss:1.88254\n",
      "[7]\ttrain-mlogloss:1.7903\teval-mlogloss:1.81012\n",
      "[8]\ttrain-mlogloss:1.73062\teval-mlogloss:1.75126\n",
      "[9]\ttrain-mlogloss:1.67667\teval-mlogloss:1.69764\n",
      "[10]\ttrain-mlogloss:1.62686\teval-mlogloss:1.64816\n",
      "[11]\ttrain-mlogloss:1.58347\teval-mlogloss:1.60491\n",
      "[12]\ttrain-mlogloss:1.54526\teval-mlogloss:1.56676\n",
      "[13]\ttrain-mlogloss:1.5097\teval-mlogloss:1.53164\n",
      "[14]\ttrain-mlogloss:1.47852\teval-mlogloss:1.50091\n",
      "[15]\ttrain-mlogloss:1.4491\teval-mlogloss:1.47187\n",
      "[16]\ttrain-mlogloss:1.42437\teval-mlogloss:1.44772\n",
      "[17]\ttrain-mlogloss:1.40017\teval-mlogloss:1.42392\n",
      "[18]\ttrain-mlogloss:1.37815\teval-mlogloss:1.40214\n",
      "[19]\ttrain-mlogloss:1.35801\teval-mlogloss:1.38243\n",
      "[20]\ttrain-mlogloss:1.33967\teval-mlogloss:1.36419\n",
      "[21]\ttrain-mlogloss:1.32261\teval-mlogloss:1.34743\n",
      "[22]\ttrain-mlogloss:1.30671\teval-mlogloss:1.33198\n",
      "[23]\ttrain-mlogloss:1.29188\teval-mlogloss:1.31768\n",
      "[24]\ttrain-mlogloss:1.27771\teval-mlogloss:1.30382\n",
      "[25]\ttrain-mlogloss:1.26402\teval-mlogloss:1.29045\n",
      "[26]\ttrain-mlogloss:1.2515\teval-mlogloss:1.27821\n",
      "[27]\ttrain-mlogloss:1.24025\teval-mlogloss:1.26718\n",
      "[28]\ttrain-mlogloss:1.2292\teval-mlogloss:1.25629\n",
      "[29]\ttrain-mlogloss:1.21916\teval-mlogloss:1.24664\n",
      "[30]\ttrain-mlogloss:1.2097\teval-mlogloss:1.23765\n",
      "[31]\ttrain-mlogloss:1.20117\teval-mlogloss:1.22948\n",
      "[32]\ttrain-mlogloss:1.19273\teval-mlogloss:1.22129\n",
      "[33]\ttrain-mlogloss:1.18499\teval-mlogloss:1.21387\n",
      "[34]\ttrain-mlogloss:1.17783\teval-mlogloss:1.20696\n",
      "[35]\ttrain-mlogloss:1.17131\teval-mlogloss:1.20078\n",
      "[36]\ttrain-mlogloss:1.16475\teval-mlogloss:1.19476\n",
      "[37]\ttrain-mlogloss:1.15886\teval-mlogloss:1.18919\n",
      "[38]\ttrain-mlogloss:1.15308\teval-mlogloss:1.18381\n",
      "[39]\ttrain-mlogloss:1.14785\teval-mlogloss:1.1789\n",
      "[40]\ttrain-mlogloss:1.14292\teval-mlogloss:1.17453\n",
      "[41]\ttrain-mlogloss:1.13802\teval-mlogloss:1.16998\n",
      "[42]\ttrain-mlogloss:1.13344\teval-mlogloss:1.1658\n",
      "[43]\ttrain-mlogloss:1.12901\teval-mlogloss:1.1618\n",
      "[44]\ttrain-mlogloss:1.12466\teval-mlogloss:1.15794\n",
      "[45]\ttrain-mlogloss:1.12089\teval-mlogloss:1.15457\n",
      "[46]\ttrain-mlogloss:1.11716\teval-mlogloss:1.15122\n",
      "[47]\ttrain-mlogloss:1.11402\teval-mlogloss:1.14851\n",
      "[48]\ttrain-mlogloss:1.11082\teval-mlogloss:1.14568\n",
      "[49]\ttrain-mlogloss:1.10773\teval-mlogloss:1.14291\n",
      "[50]\ttrain-mlogloss:1.10483\teval-mlogloss:1.14041\n",
      "[51]\ttrain-mlogloss:1.10181\teval-mlogloss:1.1378\n",
      "[52]\ttrain-mlogloss:1.099\teval-mlogloss:1.13533\n",
      "[53]\ttrain-mlogloss:1.09625\teval-mlogloss:1.1331\n",
      "[54]\ttrain-mlogloss:1.09371\teval-mlogloss:1.13097\n",
      "[55]\ttrain-mlogloss:1.09137\teval-mlogloss:1.12901\n",
      "[56]\ttrain-mlogloss:1.08917\teval-mlogloss:1.12708\n",
      "[57]\ttrain-mlogloss:1.08705\teval-mlogloss:1.12531\n",
      "[58]\ttrain-mlogloss:1.08484\teval-mlogloss:1.12356\n",
      "[59]\ttrain-mlogloss:1.08281\teval-mlogloss:1.12192\n",
      "[60]\ttrain-mlogloss:1.0809\teval-mlogloss:1.12033\n",
      "[61]\ttrain-mlogloss:1.0791\teval-mlogloss:1.11894\n",
      "[62]\ttrain-mlogloss:1.07725\teval-mlogloss:1.11757\n",
      "[63]\ttrain-mlogloss:1.0756\teval-mlogloss:1.11623\n",
      "[64]\ttrain-mlogloss:1.0739\teval-mlogloss:1.11498\n",
      "[65]\ttrain-mlogloss:1.07237\teval-mlogloss:1.11374\n",
      "[66]\ttrain-mlogloss:1.07085\teval-mlogloss:1.11266\n",
      "[67]\ttrain-mlogloss:1.06939\teval-mlogloss:1.11158\n",
      "[68]\ttrain-mlogloss:1.06802\teval-mlogloss:1.11048\n",
      "[69]\ttrain-mlogloss:1.06654\teval-mlogloss:1.10945\n",
      "[70]\ttrain-mlogloss:1.06519\teval-mlogloss:1.10847\n",
      "[71]\ttrain-mlogloss:1.06393\teval-mlogloss:1.10772\n",
      "[72]\ttrain-mlogloss:1.06271\teval-mlogloss:1.10694\n",
      "[73]\ttrain-mlogloss:1.06139\teval-mlogloss:1.10612\n",
      "[74]\ttrain-mlogloss:1.06016\teval-mlogloss:1.10532\n",
      "[75]\ttrain-mlogloss:1.05894\teval-mlogloss:1.10459\n",
      "[76]\ttrain-mlogloss:1.05783\teval-mlogloss:1.10394\n",
      "[77]\ttrain-mlogloss:1.05668\teval-mlogloss:1.10325\n",
      "[78]\ttrain-mlogloss:1.05568\teval-mlogloss:1.10272\n",
      "[79]\ttrain-mlogloss:1.05463\teval-mlogloss:1.10211\n",
      "[80]\ttrain-mlogloss:1.05373\teval-mlogloss:1.10153\n",
      "[81]\ttrain-mlogloss:1.05282\teval-mlogloss:1.10103\n",
      "[82]\ttrain-mlogloss:1.05183\teval-mlogloss:1.10049\n",
      "[83]\ttrain-mlogloss:1.05086\teval-mlogloss:1.10001\n",
      "[84]\ttrain-mlogloss:1.04987\teval-mlogloss:1.09955\n",
      "[85]\ttrain-mlogloss:1.04894\teval-mlogloss:1.09902\n",
      "[86]\ttrain-mlogloss:1.04801\teval-mlogloss:1.09852\n",
      "[87]\ttrain-mlogloss:1.04727\teval-mlogloss:1.09804\n",
      "[88]\ttrain-mlogloss:1.04656\teval-mlogloss:1.09763\n",
      "[89]\ttrain-mlogloss:1.04579\teval-mlogloss:1.09723\n",
      "[90]\ttrain-mlogloss:1.04507\teval-mlogloss:1.09686\n",
      "[91]\ttrain-mlogloss:1.04422\teval-mlogloss:1.09653\n",
      "[92]\ttrain-mlogloss:1.04335\teval-mlogloss:1.09612\n",
      "[93]\ttrain-mlogloss:1.0426\teval-mlogloss:1.09577\n",
      "[94]\ttrain-mlogloss:1.04182\teval-mlogloss:1.09547\n",
      "[95]\ttrain-mlogloss:1.04114\teval-mlogloss:1.09519\n",
      "[96]\ttrain-mlogloss:1.04041\teval-mlogloss:1.0949\n",
      "[97]\ttrain-mlogloss:1.03972\teval-mlogloss:1.09465\n",
      "[98]\ttrain-mlogloss:1.03905\teval-mlogloss:1.09439\n",
      "[99]\ttrain-mlogloss:1.03828\teval-mlogloss:1.09419\n",
      "[100]\ttrain-mlogloss:1.03761\teval-mlogloss:1.09398\n",
      "[101]\ttrain-mlogloss:1.03688\teval-mlogloss:1.09377\n",
      "[102]\ttrain-mlogloss:1.03617\teval-mlogloss:1.09357\n",
      "[103]\ttrain-mlogloss:1.03549\teval-mlogloss:1.09329\n",
      "[104]\ttrain-mlogloss:1.03489\teval-mlogloss:1.09307\n",
      "[105]\ttrain-mlogloss:1.03418\teval-mlogloss:1.09287\n",
      "[106]\ttrain-mlogloss:1.03359\teval-mlogloss:1.09268\n",
      "[107]\ttrain-mlogloss:1.03296\teval-mlogloss:1.09251\n",
      "[108]\ttrain-mlogloss:1.03236\teval-mlogloss:1.09236\n",
      "[109]\ttrain-mlogloss:1.03176\teval-mlogloss:1.09217\n",
      "[110]\ttrain-mlogloss:1.03116\teval-mlogloss:1.092\n",
      "[111]\ttrain-mlogloss:1.03049\teval-mlogloss:1.09187\n",
      "[112]\ttrain-mlogloss:1.02967\teval-mlogloss:1.09168\n",
      "[113]\ttrain-mlogloss:1.02903\teval-mlogloss:1.0916\n",
      "[114]\ttrain-mlogloss:1.02852\teval-mlogloss:1.09145\n",
      "[115]\ttrain-mlogloss:1.02794\teval-mlogloss:1.0913\n",
      "[116]\ttrain-mlogloss:1.02737\teval-mlogloss:1.09118\n",
      "[117]\ttrain-mlogloss:1.02666\teval-mlogloss:1.09104\n",
      "[118]\ttrain-mlogloss:1.02604\teval-mlogloss:1.09091\n",
      "[119]\ttrain-mlogloss:1.02549\teval-mlogloss:1.09088\n",
      "[120]\ttrain-mlogloss:1.02494\teval-mlogloss:1.09073\n",
      "[121]\ttrain-mlogloss:1.02428\teval-mlogloss:1.09063\n",
      "[122]\ttrain-mlogloss:1.02381\teval-mlogloss:1.09056\n",
      "[123]\ttrain-mlogloss:1.02323\teval-mlogloss:1.09048\n",
      "[124]\ttrain-mlogloss:1.02262\teval-mlogloss:1.09031\n",
      "[125]\ttrain-mlogloss:1.02179\teval-mlogloss:1.09021\n",
      "[126]\ttrain-mlogloss:1.02106\teval-mlogloss:1.09008\n",
      "[127]\ttrain-mlogloss:1.02033\teval-mlogloss:1.08991\n",
      "[128]\ttrain-mlogloss:1.01962\teval-mlogloss:1.08975\n",
      "[129]\ttrain-mlogloss:1.01916\teval-mlogloss:1.08967\n",
      "[130]\ttrain-mlogloss:1.01856\teval-mlogloss:1.08961\n",
      "[131]\ttrain-mlogloss:1.01802\teval-mlogloss:1.08954\n",
      "[132]\ttrain-mlogloss:1.01742\teval-mlogloss:1.08947\n",
      "[133]\ttrain-mlogloss:1.01692\teval-mlogloss:1.08943\n",
      "[134]\ttrain-mlogloss:1.01619\teval-mlogloss:1.08933\n",
      "[135]\ttrain-mlogloss:1.01556\teval-mlogloss:1.08927\n",
      "[136]\ttrain-mlogloss:1.01493\teval-mlogloss:1.08925\n",
      "[137]\ttrain-mlogloss:1.0144\teval-mlogloss:1.08921\n",
      "[138]\ttrain-mlogloss:1.01376\teval-mlogloss:1.08919\n",
      "[139]\ttrain-mlogloss:1.01329\teval-mlogloss:1.08908\n",
      "[140]\ttrain-mlogloss:1.01256\teval-mlogloss:1.08901\n",
      "[141]\ttrain-mlogloss:1.01206\teval-mlogloss:1.08897\n",
      "[142]\ttrain-mlogloss:1.01152\teval-mlogloss:1.08893\n",
      "[143]\ttrain-mlogloss:1.01094\teval-mlogloss:1.08892\n",
      "[144]\ttrain-mlogloss:1.01045\teval-mlogloss:1.08888\n",
      "[145]\ttrain-mlogloss:1.00997\teval-mlogloss:1.08883\n",
      "[146]\ttrain-mlogloss:1.00936\teval-mlogloss:1.08877\n",
      "[147]\ttrain-mlogloss:1.0088\teval-mlogloss:1.08871\n",
      "[148]\ttrain-mlogloss:1.00822\teval-mlogloss:1.0887\n",
      "[149]\ttrain-mlogloss:1.00766\teval-mlogloss:1.08867\n",
      "[150]\ttrain-mlogloss:1.00697\teval-mlogloss:1.08856\n",
      "[151]\ttrain-mlogloss:1.00639\teval-mlogloss:1.08855\n",
      "[152]\ttrain-mlogloss:1.00586\teval-mlogloss:1.08856\n",
      "[153]\ttrain-mlogloss:1.00531\teval-mlogloss:1.08847\n",
      "[154]\ttrain-mlogloss:1.00488\teval-mlogloss:1.08846\n",
      "[155]\ttrain-mlogloss:1.00426\teval-mlogloss:1.08847\n",
      "[156]\ttrain-mlogloss:1.00359\teval-mlogloss:1.08838\n",
      "[157]\ttrain-mlogloss:1.00296\teval-mlogloss:1.08835\n",
      "[158]\ttrain-mlogloss:1.00248\teval-mlogloss:1.08833\n",
      "[159]\ttrain-mlogloss:1.00188\teval-mlogloss:1.08829\n",
      "[160]\ttrain-mlogloss:1.00142\teval-mlogloss:1.08828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[161]\ttrain-mlogloss:1.00085\teval-mlogloss:1.08825\n",
      "[162]\ttrain-mlogloss:1.00026\teval-mlogloss:1.0882\n",
      "[163]\ttrain-mlogloss:0.999756\teval-mlogloss:1.08825\n",
      "[164]\ttrain-mlogloss:0.999207\teval-mlogloss:1.0882\n",
      "[165]\ttrain-mlogloss:0.998629\teval-mlogloss:1.08815\n",
      "[166]\ttrain-mlogloss:0.998034\teval-mlogloss:1.08812\n",
      "[167]\ttrain-mlogloss:0.997484\teval-mlogloss:1.08807\n",
      "[168]\ttrain-mlogloss:0.996733\teval-mlogloss:1.08804\n",
      "[169]\ttrain-mlogloss:0.996099\teval-mlogloss:1.08798\n",
      "[170]\ttrain-mlogloss:0.995525\teval-mlogloss:1.08791\n",
      "[171]\ttrain-mlogloss:0.995027\teval-mlogloss:1.08789\n",
      "[172]\ttrain-mlogloss:0.994355\teval-mlogloss:1.08792\n",
      "[173]\ttrain-mlogloss:0.993913\teval-mlogloss:1.08795\n",
      "[174]\ttrain-mlogloss:0.993255\teval-mlogloss:1.0879\n",
      "[175]\ttrain-mlogloss:0.992711\teval-mlogloss:1.08793\n",
      "[176]\ttrain-mlogloss:0.992209\teval-mlogloss:1.08791\n",
      "[177]\ttrain-mlogloss:0.991645\teval-mlogloss:1.08794\n",
      "[178]\ttrain-mlogloss:0.991089\teval-mlogloss:1.08799\n",
      "[179]\ttrain-mlogloss:0.990435\teval-mlogloss:1.08796\n",
      "[180]\ttrain-mlogloss:0.989951\teval-mlogloss:1.08797\n",
      "[181]\ttrain-mlogloss:0.989464\teval-mlogloss:1.08795\n",
      "[182]\ttrain-mlogloss:0.988891\teval-mlogloss:1.08796\n",
      "[183]\ttrain-mlogloss:0.988284\teval-mlogloss:1.08794\n",
      "[184]\ttrain-mlogloss:0.987663\teval-mlogloss:1.08793\n",
      "[185]\ttrain-mlogloss:0.987141\teval-mlogloss:1.08795\n",
      "[186]\ttrain-mlogloss:0.986696\teval-mlogloss:1.08795\n",
      "[187]\ttrain-mlogloss:0.985965\teval-mlogloss:1.0879\n",
      "[188]\ttrain-mlogloss:0.985343\teval-mlogloss:1.08787\n",
      "[189]\ttrain-mlogloss:0.984865\teval-mlogloss:1.08787\n",
      "[190]\ttrain-mlogloss:0.984236\teval-mlogloss:1.08784\n",
      "[191]\ttrain-mlogloss:0.983644\teval-mlogloss:1.08786\n",
      "[192]\ttrain-mlogloss:0.983228\teval-mlogloss:1.08784\n",
      "[193]\ttrain-mlogloss:0.982596\teval-mlogloss:1.08783\n",
      "[194]\ttrain-mlogloss:0.982011\teval-mlogloss:1.08781\n",
      "[195]\ttrain-mlogloss:0.981455\teval-mlogloss:1.08779\n",
      "[196]\ttrain-mlogloss:0.98086\teval-mlogloss:1.0878\n",
      "[197]\ttrain-mlogloss:0.980195\teval-mlogloss:1.08769\n",
      "[198]\ttrain-mlogloss:0.9797\teval-mlogloss:1.08763\n",
      "[199]\ttrain-mlogloss:0.979266\teval-mlogloss:1.08763\n",
      "[200]\ttrain-mlogloss:0.978713\teval-mlogloss:1.08764\n",
      "[201]\ttrain-mlogloss:0.978213\teval-mlogloss:1.08762\n",
      "[202]\ttrain-mlogloss:0.977865\teval-mlogloss:1.08763\n",
      "[203]\ttrain-mlogloss:0.97728\teval-mlogloss:1.08762\n",
      "[204]\ttrain-mlogloss:0.976864\teval-mlogloss:1.0876\n",
      "[205]\ttrain-mlogloss:0.976402\teval-mlogloss:1.08766\n",
      "[206]\ttrain-mlogloss:0.975864\teval-mlogloss:1.08766\n",
      "[207]\ttrain-mlogloss:0.97537\teval-mlogloss:1.08767\n",
      "[208]\ttrain-mlogloss:0.974955\teval-mlogloss:1.0877\n",
      "[209]\ttrain-mlogloss:0.974492\teval-mlogloss:1.08768\n",
      "[210]\ttrain-mlogloss:0.974007\teval-mlogloss:1.08772\n",
      "[211]\ttrain-mlogloss:0.973547\teval-mlogloss:1.08768\n",
      "[212]\ttrain-mlogloss:0.972964\teval-mlogloss:1.08771\n",
      "[213]\ttrain-mlogloss:0.972469\teval-mlogloss:1.08768\n",
      "[214]\ttrain-mlogloss:0.971953\teval-mlogloss:1.08763\n",
      "[215]\ttrain-mlogloss:0.971448\teval-mlogloss:1.08761\n",
      "[216]\ttrain-mlogloss:0.970913\teval-mlogloss:1.08762\n",
      "[217]\ttrain-mlogloss:0.970446\teval-mlogloss:1.08759\n",
      "[218]\ttrain-mlogloss:0.969869\teval-mlogloss:1.08756\n",
      "[219]\ttrain-mlogloss:0.96948\teval-mlogloss:1.08758\n",
      "[220]\ttrain-mlogloss:0.969001\teval-mlogloss:1.08757\n",
      "[221]\ttrain-mlogloss:0.968563\teval-mlogloss:1.08759\n",
      "[222]\ttrain-mlogloss:0.968048\teval-mlogloss:1.08762\n",
      "[223]\ttrain-mlogloss:0.96746\teval-mlogloss:1.08761\n",
      "[224]\ttrain-mlogloss:0.966956\teval-mlogloss:1.08757\n",
      "[225]\ttrain-mlogloss:0.9664\teval-mlogloss:1.08759\n",
      "[226]\ttrain-mlogloss:0.96595\teval-mlogloss:1.08759\n",
      "[227]\ttrain-mlogloss:0.965428\teval-mlogloss:1.08756\n",
      "[228]\ttrain-mlogloss:0.964887\teval-mlogloss:1.08754\n",
      "[229]\ttrain-mlogloss:0.964463\teval-mlogloss:1.08755\n",
      "[230]\ttrain-mlogloss:0.964033\teval-mlogloss:1.08758\n",
      "[231]\ttrain-mlogloss:0.963515\teval-mlogloss:1.08757\n",
      "[232]\ttrain-mlogloss:0.96301\teval-mlogloss:1.08758\n",
      "[233]\ttrain-mlogloss:0.962471\teval-mlogloss:1.08761\n",
      "[234]\ttrain-mlogloss:0.962012\teval-mlogloss:1.08759\n",
      "[235]\ttrain-mlogloss:0.961479\teval-mlogloss:1.08767\n",
      "[236]\ttrain-mlogloss:0.960974\teval-mlogloss:1.08769\n",
      "[237]\ttrain-mlogloss:0.960479\teval-mlogloss:1.08772\n",
      "[238]\ttrain-mlogloss:0.960093\teval-mlogloss:1.08771\n",
      "[239]\ttrain-mlogloss:0.959612\teval-mlogloss:1.08771\n",
      "[240]\ttrain-mlogloss:0.959127\teval-mlogloss:1.08767\n",
      "[241]\ttrain-mlogloss:0.958595\teval-mlogloss:1.08765\n",
      "[242]\ttrain-mlogloss:0.958241\teval-mlogloss:1.08766\n",
      "[243]\ttrain-mlogloss:0.957767\teval-mlogloss:1.08768\n",
      "[244]\ttrain-mlogloss:0.9573\teval-mlogloss:1.08769\n",
      "[245]\ttrain-mlogloss:0.95677\teval-mlogloss:1.0877\n",
      "[246]\ttrain-mlogloss:0.956344\teval-mlogloss:1.08773\n",
      "[247]\ttrain-mlogloss:0.955817\teval-mlogloss:1.08778\n",
      "[248]\ttrain-mlogloss:0.955292\teval-mlogloss:1.08779\n",
      "Stopping. Best iteration:\n",
      "[228]\ttrain-mlogloss:0.964887\teval-mlogloss:1.08754\n",
      "\n",
      "CPU times: user 36min 21s, sys: 336 ms, total: 36min 21s\n",
      "Wall time: 9min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "watch_list = [(dtrn, 'train'), (dvld, 'eval')]\n",
    "model = xgb.train(param, dtrn, num_boost_round=1000, evals=watch_list, early_stopping_rounds=20)\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open(\"xgb.baseline.pkl\", \"wb\"))\n",
    "best_ntree_limit = model.best_ntree_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=7, default=0.0):\n",
    "    # MAP@7\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "        \n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    \n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / ( i + 1.0)\n",
    "            \n",
    "    if not actual:\n",
    "        return default\n",
    "    \n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=7, default=0.0):\n",
    "    return np.mean([apk(a,p,k,default) for a,p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04266379915553903\n",
      "0.036435336167892435\n"
     ]
    }
   ],
   "source": [
    "vld = trn[trn['fecha_dato'] == vld_date]\n",
    "ncodpers_vld = vld.as_matrix(columns=['ncodpers'])\n",
    "\n",
    "for prod in prods:\n",
    "    prev = prod + '_prev'\n",
    "    padd = prod + '_add'\n",
    "    \n",
    "    vld[padd] = vld[prod] - vld[prev]\n",
    "\n",
    "add_vld = vld.as_matrix(columns=[prod + '_add' for  prod in prods])\n",
    "add_vld_list = [list() for i in range(len(ncodpers_vld))]\n",
    "\n",
    "count_vld = 0\n",
    "for ncodper in range(len(ncodpers_vld)):\n",
    "    for prod in range(len(prods)):\n",
    "        if add_vld[ncodper, prod] > 0:\n",
    "            add_vld_list[ncodper].append(prod)\n",
    "            count_vld += 1\n",
    "            \n",
    "print(mapk(add_vld_list, add_vld_list, 7, 0.0))\n",
    "\n",
    "X_vld = vld.as_matrix(columns = features)\n",
    "Y_vld = vld.as_matrix(columns=['y'])\n",
    "\n",
    "dvld = xgb.DMatrix(X_vld, label=Y_vld, feature_names=features)\n",
    "preds_vld = model.predict(dvld, ntree_limit=best_ntree_limit)\n",
    "\n",
    "preds_vld = preds_vld - vld.as_matrix(columns=[prod + '_prev' for prod in prods])\n",
    "\n",
    "result_vld = []\n",
    "for ncodper, pred in zip(ncodpers_vld, preds_vld):\n",
    "    y_prods = [(y,p,ip) for y,p,ip in zip(pred, prods, range(len(prods)))]\n",
    "    y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
    "    result_vld.append([ip for y,p,ip in y_prods])\n",
    "    \n",
    "print(mapk(add_vld_list, result_vld, 7,0.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.70996\n",
      "[1]\ttrain-mlogloss:2.45557\n",
      "[2]\ttrain-mlogloss:2.28324\n",
      "[3]\ttrain-mlogloss:2.14185\n",
      "[4]\ttrain-mlogloss:2.02824\n",
      "[5]\ttrain-mlogloss:1.93225\n",
      "[6]\ttrain-mlogloss:1.85379\n",
      "[7]\ttrain-mlogloss:1.7844\n",
      "[8]\ttrain-mlogloss:1.72256\n",
      "[9]\ttrain-mlogloss:1.66696\n",
      "[10]\ttrain-mlogloss:1.61826\n",
      "[11]\ttrain-mlogloss:1.57553\n",
      "[12]\ttrain-mlogloss:1.5374\n",
      "[13]\ttrain-mlogloss:1.50176\n",
      "[14]\ttrain-mlogloss:1.47008\n",
      "[15]\ttrain-mlogloss:1.44189\n",
      "[16]\ttrain-mlogloss:1.41677\n",
      "[17]\ttrain-mlogloss:1.39205\n"
     ]
    }
   ],
   "source": [
    "X_all = XY.as_matrix(columns=features)\n",
    "Y_all = XY.as_matrix(columns=['y'])\n",
    "dall = xgb.DMatrix(X_all, label=Y_all, feature_names=features)\n",
    "watch_list = [(dall, 'train')]\n",
    "\n",
    "best_ntree_limit = int(best_ntree_limit * (len(XY_trn) + len(XY_vld)) / len(XY_trn))\n",
    "\n",
    "model = xgb.train(param, dall, num_boost_round=best_ntree_limit, evals=watch_list)\n",
    "\n",
    "print(\"Feature importance:\")\n",
    "for kv in sorted([(k, v) for k,v in model.get_fscore().items()], key=lambda kv: kv[1], reverse=True):\n",
    "    print(kv)\n",
    "    \n",
    "X_tst = tst.as_matrix(columns=feature)\n",
    "dtst = xgb.DMatrix(X_tst, feature_names=features)\n",
    "preds_tst = model.predict(dtst, ntree_limit=best_ntree_limit)\n",
    "ncodpers_tst = tst.as_matrix(columns=['ncodpers'])\n",
    "preds_tst = preds_tst - tst.as_matrix(columns=[prod + '_prev' for prod in prods])\n",
    "\n",
    "\n",
    "sumit_file = open('xgb.baseline.2015-06-28', 'w')\n",
    "submit_file.write('ncodpers,added_products\\n')\n",
    "for ncodper, pred in zip(ncodpers_tst, preds_tst):\n",
    "    y_prods = [(y,p,ip) for y,pmip in zip(pred, prods, range(len(prods)))]\n",
    "    y_prods = sorted(y_prods, key=lambda a: a[0], reverse = True)[:7]\n",
    "    y_prods = [p for y,p, ip in y_prods]\n",
    "    submit_file.write('{},{}\\n'.format(int(ncodper), ' '.join(y_prods)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
